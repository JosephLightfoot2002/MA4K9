{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf50006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, d_model=4,Qbias=False,Vbias=True,Mask=False,OnlyAtt=True,n_heads=1):\n",
    "        super(MHAttention, self).__init__()\n",
    "        self.dk=d_model//n_heads\n",
    "        self.Q=nn.Linear(d_model,d_model,bias=Qbias)\n",
    "        self.K=nn.Linear(d_model,d_model,bias=Qbias)\n",
    "        self.V=nn.Linear(d_model,d_model,bias=Vbias)\n",
    "        self.O=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.Mask=Mask\n",
    "       \n",
    "        self.OnlyAtt=OnlyAtt\n",
    "        self.n_heads=n_heads\n",
    "        self.d_model=d_model\n",
    "    def forward(self, src,mask=None):\n",
    "        # Q=self.Q(src)\n",
    "       \n",
    "        # K=self.K(src)\n",
    "        # V=self.V(src)\n",
    "        # output=torch.zeros(size=src.shape).to(device)\n",
    "        # for i in range(self.n_heads):\n",
    "\n",
    "    \n",
    "        #     O=torch.matmul(Q[:,:,i*self.n_heads:(i+1)*self.n_heads],torch.transpose(K[:,:,i*self.n_heads:(i+1)*self.n_heads],dim0=1,dim1=2))/np.sqrt(self.dk)\n",
    "\n",
    "        #     if self.Mask:\n",
    "        #         #n\n",
    "        #         seq_len=src.size(1)\n",
    "        #         #Upper triangular Matrix of Trues\n",
    "        #         mask = torch.triu(torch.ones(seq_len, seq_len, device=src.device), diagonal=1).bool()\n",
    "\n",
    "        #         O=O.masked_fill(mask,float('-inf'))\n",
    "            \n",
    "        #     att=torch.softmax(O,dim=2)\n",
    "       \n",
    "        #     output[:,:,i*self.n_heads:(i+1)*self.n_heads]=torch.matmul(att,V[:,:,i*self.n_heads:(i+1)*self.n_heads])\n",
    "        \n",
    "        # #V=att_out\n",
    "        # if self.OnlyAtt:\n",
    "        #     return output[:,0]\n",
    "        # else:\n",
    "        #     return output\n",
    "        batch_size, seq_len, _ = src.shape\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.Q(src).view(batch_size, seq_len, self.n_heads, self.dk).transpose(1, 2)  # (B, H, S, D_k)\n",
    "        K = self.K(src).view(batch_size, seq_len, self.n_heads, self.dk).transpose(1, 2)  \n",
    "        V = self.V(src).view(batch_size, seq_len, self.n_heads, self.dk).transpose(1, 2)  \n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dk)  # (B, H, S, S)\n",
    "\n",
    "        # Apply mask if needed\n",
    "        if mask is not None:\n",
    "            # Ensure mask is broadcastable to scores shape.\n",
    "            # Mask should have -inf where you want to block attention.\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, H, S, S)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn, V)  # (B, H, S, D_k)\n",
    "\n",
    "        # Reshape & project output\n",
    "        if self.n_heads>1:\n",
    "            context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (B, S, D)\n",
    "        output = self.O(context)\n",
    "\n",
    "        return output if not self.OnlyAtt else output[:, 0]\n",
    "\n",
    "class MHAttentionApproximatorRes(nn.Module):\n",
    "    def __init__(self,d_model=1,Qbias=False,Vbias=True,i_dim=1,o_dim=1,n_heads=1,mask=None):\n",
    "        super(MHAttentionApproximatorRes,self).__init__()\n",
    "        self.MHAttention=MHAttention(d_model,Qbias=Qbias,Vbias=Vbias,OnlyAtt=True,n_heads=n_heads,Mask=mask)\n",
    "        self.embedding=nn.Linear(i_dim,d_model)\n",
    "        \n",
    "        self.O=nn.Linear(d_model,i_dim)\n",
    "\n",
    "    def forward(self,src,mask=None):\n",
    "        emb=self.embedding(src)\n",
    "        \n",
    "        emb=self.MHAttention(emb,mask=mask)\n",
    "        emb=self.O(emb)\n",
    "\n",
    "        return src+emb\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(MLP,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,output_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "class TLayerSpec(nn.Module):\n",
    "    def __init__(self,d_model,Qbias=False,Vbias=True,n_heads=1,Mask=False,OnlyAtt=False,dimff=8,d_in=1,d_out=1,mask=None):\n",
    "        super(TLayerSpec,self).__init__()\n",
    "        self.attention=MHAttentionApproximatorRes(d_model,Qbias=Qbias,Vbias=Vbias,n_heads=n_heads,mask=mask,i_dim=d_in)\n",
    "        self.mlp=MLP(d_in,dimff,d_out)\n",
    "\n",
    "    def forward(self,src,mask=None):\n",
    "        attn_out=self.attention(src,mask=mask)\n",
    "        n1=src+attn_out\n",
    "        mlp1=self.mlp(n1)\n",
    "        return mlp1\n",
    "    \n",
    "class MyTransformerSpec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 Qbias=False,Vbias=True,n_heads=1,Mask=False,OnlyAtt=True,\n",
    "                 layers=1,d_model=1,\n",
    "                 dimFeedForward=[[1,16,1]]):\n",
    "        super(MyTransformerSpec,self).__init__()\n",
    "        self.tlayers= nn.ModuleList([\n",
    "            TLayerSpec(d_in=dimFeedForward[i][0],d_model=d_model,\n",
    "                Qbias=Qbias,Vbias=Vbias,n_heads=n_heads,\n",
    "                Mask=Mask,OnlyAtt=False,dimff=dimFeedForward[i][1],d_out=dimFeedForward[i][2])\n",
    "                for i in range(layers)\n",
    "        ])\n",
    "        self.layers=layers\n",
    "        \n",
    "\n",
    "    def forward(self, x, X):\n",
    "        # Concatenate x (external token) with token matrix X along dimension 1.\n",
    "        # x: (batch_size, 1, d_model) and X: (batch_size, seq_len, d_model)\n",
    "        ce = torch.cat([x, X], dim=1).to(torch.float32)  # Now shape: (batch_size, total_seq_len, d_model)\n",
    "        \n",
    "        # Create an attention mask so that the external token (index 0) does not attend to any tokens in X.\n",
    "        batch_size, total_len, _ = ce.shape  # total_len = 1 + seq_len\n",
    "        # Create a mask of shape (total_len, total_len)\n",
    "        # We want to block attention from token at index 0 to indices 1...total_len-1.\n",
    "        mask = torch.zeros(total_len, total_len, dtype=torch.bool, device=ce.device)\n",
    "        mask[0, 1:] = True\n",
    "        # Expand the mask over the batch dimension\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "   \n",
    "        # Pass the mask into each transformer layer\n",
    "        for i in range(self.layers):\n",
    "            ce = self.tlayers[i](ce, mask=mask)\n",
    "        \n",
    "        return ce[:,0,:]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c5c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_positive_definite_matrix(n):\n",
    "    # Step 1: Generate a random matrix\n",
    "    A = np.random.randn(n, n)\n",
    "\n",
    "    # Step 2: Perform QR decomposition to get an orthonormal matrix Q\n",
    "    Q, _ = np.linalg.qr(A)\n",
    "\n",
    "    # Step 3: Generate a diagonal matrix of positive eigenvalues\n",
    "    eigenvalues = np.abs(np.random.randn(n)) + 1  # Ensuring eigenvalues are positive\n",
    "\n",
    "    # Step 4: Create a diagonal matrix of eigenvalues\n",
    "    Lambda = np.diag(eigenvalues)\n",
    "\n",
    "    # Step 5: Compute the positive definite matrix A = Q * Lambda * Q.T\n",
    "    positive_definite_matrix = np.dot(Q, np.dot(Lambda, Q.T))\n",
    "\n",
    "    return positive_definite_matrix\n",
    "\n",
    "# Example usage\n",
    "n = 4  # Size of the matrix\n",
    "matrix = random_positive_definite_matrix(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd9f497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000,d=1,n=10,V=torch.zeros(1)):\n",
    "        self.data = torch.tensor(np.random.normal(0,2,size=[num_samples,n+1,d]),dtype=torch.float64)\n",
    "        self.Y=[]\n",
    "        for i in range(num_samples):\n",
    "            x=self.data[i][0]\n",
    "            X=self.data[i][1:]\n",
    "            current=torch.zeros(1)\n",
    "           \n",
    "            v=torch.matmul(V,x)\n",
    "            \n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    current+=torch.abs(torch.matmul(v,X[i]-X[j]))\n",
    "                \n",
    "                \n",
    "\n",
    "            self.Y.append(current/(n**2))\n",
    "                    \n",
    "        self.V=V\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.Y[idx]\n",
    "        return x[0],x[1:],y\n",
    "dataset=ContextDataset(1000,4,10,torch.tensor(matrix,dtype=torch.float64))\n",
    "dataset2=ContextDataset(1000,4,10,torch.tensor(matrix,dtype=torch.float64))\n",
    "dataloader=DataLoader(dataset,100)\n",
    "dataloader2=DataLoader(dataset2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35fbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.01182022094727\n",
      "112.55735473632812\n",
      "7.600576543807984\n",
      "7.565220832824707\n",
      "7.454439640045166\n",
      "7.468077659606934\n",
      "7.372849321365356\n",
      "7.404358720779419\n",
      "7.291619920730591\n",
      "7.391693735122681\n",
      "7.189855003356934\n",
      "7.428511047363282\n",
      "7.116103315353394\n",
      "7.479602336883545\n",
      "7.047230625152588\n",
      "7.513826179504394\n",
      "7.003833293914795\n",
      "7.566576671600342\n",
      "6.964155483245849\n",
      "7.597510099411011\n",
      "6.9509388446807865\n",
      "7.628451585769653\n",
      "6.945015621185303\n",
      "7.637031984329224\n",
      "6.946006345748901\n",
      "7.639179992675781\n",
      "6.941211843490601\n",
      "7.64763879776001\n",
      "6.938714218139649\n",
      "7.644886207580567\n",
      "6.937544536590576\n",
      "7.646400308609008\n",
      "6.941845083236695\n",
      "7.645077466964722\n",
      "6.941077184677124\n",
      "7.644385528564453\n",
      "6.936041450500488\n",
      "7.653135824203491\n",
      "6.940794801712036\n",
      "7.650232267379761\n",
      "6.938742351531983\n",
      "7.645614337921143\n",
      "6.935816478729248\n",
      "7.649838829040528\n",
      "6.939172887802124\n",
      "7.651364803314209\n",
      "6.936597871780395\n",
      "7.654007911682129\n",
      "6.936481142044068\n",
      "7.64950122833252\n",
      "6.93694167137146\n",
      "7.657779884338379\n",
      "6.936551141738891\n",
      "7.650830507278442\n",
      "6.934852743148804\n",
      "7.652986335754394\n",
      "6.934442043304443\n",
      "7.6519066333770756\n",
      "6.936268091201782\n",
      "7.651203250885009\n",
      "6.932247495651245\n",
      "7.650324153900146\n",
      "6.9366662979125975\n",
      "7.654802894592285\n",
      "6.932912349700928\n",
      "7.652024841308593\n",
      "6.934908008575439\n",
      "7.654371690750122\n",
      "6.93313684463501\n",
      "7.653110218048096\n",
      "6.930493450164795\n",
      "7.651307201385498\n",
      "6.933483123779297\n",
      "7.652494096755982\n",
      "6.933188343048096\n",
      "7.651384973526001\n",
      "6.932841110229492\n",
      "7.6553205966949465\n",
      "6.934826517105103\n",
      "7.654982185363769\n",
      "6.930165958404541\n",
      "7.652342510223389\n",
      "6.928695440292358\n",
      "7.6523706912994385\n",
      "6.93135175704956\n",
      "7.655423212051391\n",
      "6.932641077041626\n",
      "7.650856924057007\n",
      "6.932556533813477\n",
      "7.655512952804566\n",
      "6.9310760498046875\n",
      "7.653902673721314\n",
      "6.930121755599975\n",
      "7.654770469665527\n",
      "6.927501106262207\n",
      "7.654142761230469\n",
      "6.932527923583985\n",
      "7.656525182723999\n",
      "6.927088403701783\n",
      "7.652907752990723\n",
      "6.931202220916748\n",
      "7.66143832206726\n",
      "6.926065254211426\n",
      "7.654530715942383\n",
      "6.930741548538208\n",
      "7.662351322174072\n",
      "6.930487155914307\n",
      "7.657169818878174\n",
      "6.929624414443969\n",
      "7.655877017974854\n",
      "6.927486801147461\n",
      "7.657338619232178\n",
      "6.927960920333862\n",
      "7.661235380172729\n",
      "6.926660490036011\n",
      "7.656393098831177\n",
      "6.9263934135437015\n",
      "7.660239934921265\n",
      "6.9299482822418215\n",
      "7.665429019927979\n",
      "6.9299109935760494\n",
      "7.661137294769287\n",
      "6.924510288238525\n",
      "7.662597179412842\n",
      "6.92599458694458\n",
      "7.66230001449585\n",
      "6.926170015335083\n",
      "7.660530233383179\n",
      "6.925421810150146\n",
      "7.659026861190796\n",
      "6.927792167663574\n",
      "7.658990526199341\n",
      "6.926065492630005\n",
      "7.6566637516021725\n",
      "6.927385425567627\n",
      "7.66134123802185\n",
      "6.927471446990967\n",
      "7.663293647766113\n",
      "6.926889944076538\n",
      "7.663854932785034\n",
      "6.925923299789429\n",
      "7.659261178970337\n",
      "6.925531244277954\n",
      "7.664258623123169\n",
      "6.928350782394409\n",
      "7.66591067314148\n",
      "6.926957798004151\n",
      "7.665994644165039\n",
      "6.924581050872803\n",
      "7.659134578704834\n",
      "6.927268695831299\n",
      "7.663155221939087\n",
      "6.9233651638031\n",
      "7.665347242355347\n",
      "6.925633287429809\n",
      "7.6646459102630615\n",
      "6.926075172424317\n",
      "7.671441841125488\n",
      "6.92376127243042\n",
      "7.662151765823364\n",
      "6.923300552368164\n",
      "7.665686082839966\n",
      "6.924270725250244\n",
      "7.666512489318848\n",
      "6.9226056098937985\n",
      "7.6634961605072025\n",
      "6.921907806396485\n",
      "7.665984773635865\n",
      "6.921280097961426\n",
      "7.660920000076294\n",
      "6.922018766403198\n",
      "7.666863584518433\n",
      "6.921113348007202\n",
      "7.664141702651977\n",
      "6.918488836288452\n",
      "7.667128705978394\n",
      "6.919272422790527\n",
      "7.675711822509766\n",
      "6.918685340881348\n",
      "7.679338121414185\n",
      "6.918329906463623\n",
      "7.675061225891113\n",
      "6.919182968139649\n",
      "7.6751049041748045\n",
      "6.917743778228759\n",
      "7.672279167175293\n",
      "6.916712379455566\n",
      "7.670719766616822\n",
      "6.916327476501465\n",
      "7.676721620559692\n",
      "6.9163164615631105\n",
      "7.675491237640381\n",
      "6.917557954788208\n",
      "7.675850057601929\n",
      "6.917806005477905\n",
      "7.668985986709595\n",
      "6.915994119644165\n",
      "7.670956945419311\n",
      "6.914643049240112\n",
      "7.668996095657349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch import optim\n",
    "epochs=10000\n",
    "#with sdpa_kernel([SDPBackend.MATH]):\n",
    "model = MyTransformerSpec(dimFeedForward=[[4,16,1]],n_heads=1,layers=1,d_model=4).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "TrainErrorAdam = []\n",
    "TestErrorAdam=[]\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x,tok, targets = batch\n",
    "        x,tok, targets = x.unsqueeze(1).to(device),tok.to(device), targets.to(device)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x,tok)  # (batch_size, seq_len, vocab_size)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "\n",
    "    total_loss = 0\n",
    "    TrainErrorAdam.append(avg_train_loss)\n",
    "    if epoch%100==0:\n",
    "        print(TrainErrorAdam[-1])\n",
    "    model.eval() \n",
    "    for batch in dataloader2:\n",
    "        x,tok, targets = batch\n",
    "        x,tok, targets = x.unsqueeze(1).to(device),tok.to(device), targets.to(device)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x,tok)  # (batch_size, seq_len, vocab_size)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "    TestErrorAdam.append(avg_train_loss)\n",
    "    if epoch%100==0:\n",
    "        print(TestErrorAdam[-1])\n",
    "\n",
    "\n",
    "\n",
    "#with sdpa_kernel([SDPBackend.MATH]):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41316de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
